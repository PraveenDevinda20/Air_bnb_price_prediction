{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6b3e69",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import ast\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, median_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6482e3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "input_file = \"../data/raw/listings_detailed.csv\"\n",
    "output_file = \"../data/processed/listings_fixed.csv\"\n",
    "\n",
    "rows = []\n",
    "\n",
    "# Step 1: Read raw CSV safely\n",
    "with open(input_file, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        rows.append(row)\n",
    "\n",
    "# Step 2: Detect the correct number of columns (based on header)\n",
    "expected_cols = len(rows[0])\n",
    "print(f\"Expected columns: {expected_cols}\")\n",
    "\n",
    "# Step 3: Fix rows with wrong number of columns\n",
    "fixed_rows = []\n",
    "for i, row in enumerate(rows):\n",
    "    if len(row) != expected_cols:\n",
    "        print(f\"Fixing row {i+1}: had {len(row)} columns\")\n",
    "        if len(row) < expected_cols:\n",
    "            # Pad missing columns\n",
    "            row += [\"\"] * (expected_cols - len(row))\n",
    "        else:\n",
    "            # Merge extra columns into the last one\n",
    "            row = row[:expected_cols-1] + [\",\".join(row[expected_cols-1:])]\n",
    "    fixed_rows.append(row)\n",
    "\n",
    "# Step 4: Save fixed CSV\n",
    "with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(fixed_rows)\n",
    "\n",
    "# Step 5: Load into pandas\n",
    "data = pd.read_csv(output_file)\n",
    "print(\"DataFrame shape:\", data.shape)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32abfe53",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)  \n",
    "pd.set_option('display.max_columns', None)\n",
    "data.isna().sum().to_csv(\"../data/processed/missing_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5195aa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data.price\n",
    "data['price'] = data['price'].replace('[\\$,]', '', regex = True).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2110871",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Q1 = data[\"price\"].quantile(0.25)   \n",
    "Q3 = data[\"price\"].quantile(0.75)   \n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# outliers = data[(data[\"price\"] < lower_bound) | (data[\"price\"] > upper_bound)]\n",
    "data = data[(data['price'] >= lower_bound) & (data['price'] <= upper_bound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3a6cbf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# data[\"price_log\"] = np.log1p(data[\"price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a177a4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "drop_cols = ['id','listing_url','scrape_id','name','description',\n",
    "             'picture_url','license',\n",
    "             'host_url','host_thumbnail_url','host_picture_url']\n",
    "\n",
    "data = data.drop(columns=drop_cols, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87d6979",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for col in ['host_response_rate','host_acceptance_rate']:\n",
    "    data[col] = data[col].replace('%','', regex=True).replace('Unknown', 0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4a0732",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "num_cols = data.select_dtypes(include=['float64','int64']).columns\n",
    "data[num_cols] = data[num_cols].fillna(data[num_cols].median())\n",
    "\n",
    "cat_cols = data.select_dtypes(include='object').columns\n",
    "data[cat_cols] = data[cat_cols].fillna(\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63325575",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# plt.hist(data[\"price_log\"], bins=50)\n",
    "# plt.title(\"Distribution of Log-Transformed Prices\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3609f118",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# plt.boxplot(data[\"price_log\"], vert=False)\n",
    "# plt.title(\"Boxplot of Log-Transformed Prices\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e7ed33",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    # stats.probplot(data[\"price_log\"], dist=\"norm\", plot=plt)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c8c245",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Percentile-based winsorization\n",
    "lower_cap = data[\"price\"].quantile(0.01)\n",
    "upper_cap = data[\"price\"].quantile(0.99)\n",
    "\n",
    "data[\"price_capped\"] = data[\"price\"].clip(lower=lower_cap, upper=upper_cap)\n",
    "data[\"price_log\"] = np.log1p(data[\"price_capped\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14aa876",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(data[\"price_log\"], bins=50)\n",
    "plt.title(\"Distribution of Log-Transformed Prices\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe1832d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "stats.probplot(data[\"price_log\"], dist=\"norm\", plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a2262",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sns.histplot(data[\"price_log\"], kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e784b9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data['host_since'] = pd.to_datetime(data['host_since'], errors='coerce')\n",
    "data['host_days'] = (pd.to_datetime(\"today\") - data['host_since']).dt.days.fillna(0)\n",
    "\n",
    "data['last_review'] = pd.to_datetime(data['last_review'], errors='coerce')\n",
    "data['days_since_last_review'] = (pd.to_datetime(\"today\") - data['last_review']).dt.days.fillna(9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db274c3a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data.to_csv(r\"D:\\SLIIT\\Year 3\\Semester 2\\FDM\\Mini Project\\Air_bnb_price_prediction\\data\\processed\\new.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc8ce42",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "data['amenities'] = data['amenities'].fillna(\"\")\n",
    "\n",
    "# Count total number of amenities\n",
    "data['amenities_count'] = data['amenities'].apply(lambda x: len(x.split(',')) if isinstance(x, str) else 0)\n",
    "\n",
    "# Find most frequent amenities\n",
    "top_n = 15\n",
    "all_amenities = data['amenities'].str.split(',').explode().str.strip()\n",
    "top_amenities = all_amenities.value_counts().head(top_n).index\n",
    "\n",
    "# Create binary columns for top amenities (safe version)\n",
    "for a in top_amenities:\n",
    "    col_name = 'has_' + a.replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"/\", \"_\").lower()\n",
    "    data[col_name] = data['amenities'].str.contains(a, case=False, regex=False).fillna(False).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7320ef",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data = data.drop(columns=['amenities'])\n",
    "data= data.drop(columns=['last_scraped', 'source','host_id'])\n",
    "data = data.drop(columns=['host_name', 'host_about'])\n",
    "data = data.drop(columns=['calendar_last_scraped'])\n",
    "data = data.drop(columns=['bathrooms'])\n",
    "data = data.drop(columns=['calendar_updated', 'last_review'])\n",
    "data = data.dropna(subset=['host_since'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a307793",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# List of columns to drop\n",
    "\n",
    "drop_cols = [\n",
    "    # Price-related\n",
    "    'price',\n",
    "    'price_capped',\n",
    "\n",
    "    # Review dates\n",
    "    'first_review',\n",
    "    'last_review',\n",
    "\n",
    "    # Free-text columns\n",
    "    'name',\n",
    "    'summary',\n",
    "    'description',\n",
    "    'neighborhood_overview',\n",
    "    'notes',\n",
    "    'transit',\n",
    "    'access',\n",
    "    'interaction',\n",
    "    'house_rules'\n",
    "]\n",
    "\n",
    "# Drop if exists in the dataset\n",
    "data = data.drop(columns=[c for c in drop_cols if c in data.columns], errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc10da8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Outlier handling for minimum_nights and maximum_nights\n",
    "\n",
    "# Cap minimum_nights at 30 (anything above becomes 30)\n",
    "if 'minimum_nights' in data.columns:\n",
    "    data['minimum_nights'] = data['minimum_nights'].clip(upper=30)\n",
    "\n",
    "# Cap maximum_nights at 365 (anything above becomes 365)\n",
    "if 'maximum_nights' in data.columns:\n",
    "    data['maximum_nights'] = data['maximum_nights'].clip(upper=365)\n",
    "\n",
    "# If dataset has related min/max/avg columns, cap them too\n",
    "for col in data.columns:\n",
    "    if 'minimum_nights' in col.lower():\n",
    "        data[col] = data[col].clip(upper=30)\n",
    "    if 'maximum_nights' in col.lower():\n",
    "        data[col] = data[col].clip(upper=365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8951a9ea",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. Define categorical columns\n",
    "# -----------------------------\n",
    "categorical_low = [\n",
    "    'host_response_time', 'host_is_superhost', 'host_verifications',\n",
    "    'host_has_profile_pic', 'host_identity_verified',\n",
    "    'neighbourhood_group_cleansed', 'room_type',\n",
    "    'has_availability', 'instant_bookable'\n",
    "]\n",
    "\n",
    "categorical_high = [\n",
    "    'host_location', 'host_neighbourhood', 'neighbourhood',\n",
    "    'neighbourhood_cleansed', 'property_type', 'bathrooms_text'\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Identify numeric columns\n",
    "# -----------------------------\n",
    "numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Detect binary flags (only 0/1 values)\n",
    "binary_flags = [col for col in numeric_cols if set(data[col].dropna().unique()) <= {0,1}]\n",
    "\n",
    "# Continuous numeric = numeric but not binary\n",
    "continuous_nums = [col for col in numeric_cols if col not in binary_flags]\n",
    "\n",
    "print(\"Binary flags:\", binary_flags[:10])\n",
    "print(\"Continuous numeric:\", continuous_nums[:10])\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Label Encode high-cardinality categoricals\n",
    "# -----------------------------\n",
    "label_encoders = {}\n",
    "for col in categorical_high:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = data[col].astype(str).fillna(\"Unknown\")\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Build preprocessing pipeline\n",
    "# -----------------------------\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Continuous numeric: impute + scale\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ]), continuous_nums),\n",
    "\n",
    "        # Binary flags: keep as is\n",
    "        (\"binary\", \"passthrough\", binary_flags),\n",
    "\n",
    "        # Low-cardinality categoricals: OneHotEncode\n",
    "        (\"low_cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_low),\n",
    "\n",
    "        # High-cardinality categoricals: already label encoded\n",
    "        (\"high_cat\", \"passthrough\", categorical_high)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201a1312",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Inspect all unique values in each column\n",
    "for col in data.columns:\n",
    "    unique_vals = data[col].dropna().unique()\n",
    "    print(f\"{col}: {unique_vals[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a7945c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Function to detect t/f columns allowing unknowns\n",
    "def detect_tf_columns_with_unknown(df, unknown_vals=['Unknown']):\n",
    "    tf_cols = []\n",
    "    for col in df.columns:\n",
    "        col_vals = df[col].dropna().astype(str).str.strip().str.lower()\n",
    "        # Exclude unknown values\n",
    "        col_vals = col_vals[~col_vals.isin([val.lower() for val in unknown_vals])]\n",
    "        if col_vals.isin(['t','f']).all():\n",
    "            tf_cols.append(col)\n",
    "    return tf_cols\n",
    "\n",
    "# Detect t/f columns\n",
    "tf_cols = detect_tf_columns_with_unknown(data)\n",
    "print(\"t/f columns found:\", tf_cols)\n",
    "\n",
    "# Convert t/f to binary, set unknowns to NaN\n",
    "for col in tf_cols:\n",
    "    data[col] = data[col].astype(str).str.strip().str.lower().replace({'unknown': np.nan}).map({'t':1,'f':0}).astype(float)\n",
    "\n",
    "# Preview results\n",
    "print(data[tf_cols].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e476e3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Convert string to list\n",
    "data['host_verifications_list'] = data['host_verifications'].apply(lambda x: ast.literal_eval(x) if pd.notna(x) else [])\n",
    "\n",
    "# Define all possible verifications in the dataset\n",
    "all_verifications = set(v for lst in data['host_verifications_list'] for v in lst)\n",
    "\n",
    "# Create binary columns\n",
    "for v in all_verifications:\n",
    "    data[f'verified_{v}'] = data['host_verifications_list'].apply(lambda lst: 1 if v in lst else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccd19e9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data = data.drop(columns=['host_verifications_list', 'host_verifications'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d3d802",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Convert 'host_since' to datetime\n",
    "data['host_since'] = pd.to_datetime(data['host_since'], errors='coerce')\n",
    "\n",
    "# Feature 1: Host tenure in years\n",
    "data['host_tenure_years'] = (pd.Timestamp.now() - data['host_since']).dt.days / 365\n",
    "\n",
    "# Feature 2 (optional): Host tenure in months\n",
    "data['host_tenure_months'] = (pd.Timestamp.now() - data['host_since']).dt.days // 30\n",
    "\n",
    "# Feature 3 (optional): Extract year and month separately\n",
    "data['host_since_year'] = data['host_since'].dt.year\n",
    "data['host_since_month'] = data['host_since'].dt.month\n",
    "\n",
    "# Drop the original 'host_since' column\n",
    "data = data.drop(columns=['host_since'])\n",
    "\n",
    "# Preview the new features\n",
    "print(data[['host_tenure_years', 'host_tenure_months', 'host_since_year', 'host_since_month']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12757e49",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# One-hot encode neighbourhood_group_cleansed\n",
    "data = pd.get_dummies(data, columns=['neighbourhood_group_cleansed'], prefix='neighbourhood')\n",
    "\n",
    "# One-hot encode room_type\n",
    "data = pd.get_dummies(data, columns=['room_type'], prefix='room')\n",
    "\n",
    "# Preview the new columns\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e941b4a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Select columns that are boolean (True/False)\n",
    "bool_cols = data.select_dtypes(include='bool').columns\n",
    "\n",
    "# Convert True → 1, False → 0\n",
    "data[bool_cols] = data[bool_cols].astype(int)\n",
    "\n",
    "# Preview results\n",
    "print(data[bool_cols].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e1e63a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# One-hot encode 'host_response_time'\n",
    "host_response_dummies = pd.get_dummies(data['host_response_time'], \n",
    "                                       prefix='host_response', \n",
    "                                       dummy_na=True)  # dummy_na=True will create a column for missing values\n",
    "\n",
    "# Concatenate back to the original dataframe\n",
    "data = pd.concat([data, host_response_dummies], axis=1)\n",
    "\n",
    "# Drop the original column\n",
    "data = data.drop('host_response_time', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2896db8c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "one_hot_cols = [col for col in data.columns if col.startswith('host_response_')]\n",
    "\n",
    "# Convert bool to int\n",
    "data[one_hot_cols] = data[one_hot_cols].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c247c52f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data = data.dropna(subset=[\"host_is_superhost\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99a01a1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 1. Features and target\n",
    "# -------------------------------\n",
    "X = data.drop(\"price_log\", axis=1)\n",
    "y = data[\"price_log\"]   # winsorized + log transformed price\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Train/test split\n",
    "# -------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "# Make safe feature names for XGBoost\n",
    "X_train = X_train.rename(columns=lambda c: str(c).replace('[','').replace(']','').replace('<',''))\n",
    "X_test = X_test.rename(columns=lambda c: str(c).replace('[','').replace(']','').replace('<',''))\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Initialize XGBoost Regressor\n",
    "# -------------------------------\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=500,      # number of boosting rounds\n",
    "    learning_rate=0.05,    # step size shrinkage\n",
    "    max_depth=6,           # tree depth\n",
    "    subsample=0.8,         # row sampling\n",
    "    colsample_bytree=0.8,  # feature sampling\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Train the model\n",
    "# -------------------------------\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Predict log-price\n",
    "# -------------------------------\n",
    "y_pred_log = xgb_model.predict(X_test)\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Convert predictions back to original price\n",
    "# -------------------------------\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "y_true = np.expm1(y_test)\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Evaluate model performance\n",
    "# -------------------------------\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "medae = median_absolute_error(y_true, y_pred)\n",
    "\n",
    "print(\"XGBoost with log(price):\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "\n",
    "print(f\"R² Score: {r2:.3f}\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"Median AE: {medae:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a82e333",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'learning_rate': np.linspace(0.01, 0.1, 10),\n",
    "    'max_depth': [4, 6, 8, 10],\n",
    "    'n_estimators': [300, 400, 500, 600],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "rand_search = RandomizedSearchCV(\n",
    "    estimator=XGBRegressor(random_state=42, n_jobs=-1),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,          # 30 random combinations\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rand_search.fit(X_train, y_train)\n",
    "print(\"Best Parameters:\", rand_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c5ff28",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "xgb_tuned = XGBRegressor(\n",
    "    n_estimators=600,\n",
    "    max_depth=10,\n",
    "    learning_rate=0.03,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_tuned.fit(X_train, y_train)\n",
    "y_pred_log = xgb_tuned.predict(X_test)\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "y_true = np.expm1(y_test)\n",
    "\n",
    "# Evaluate\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "medae = median_absolute_error(y_true, y_pred)\n",
    "\n",
    "print(\"Tuned XGBoost Results:\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"R² Score: {r2:.3f}\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"Median AE: {medae:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce00968",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Feature Importance\n",
    "# -------------------------------\n",
    "importances = xgb_tuned.feature_importances_\n",
    "features = X_train.columns\n",
    "\n",
    "# Put into DataFrame for easy sorting\n",
    "feat_imp = pd.DataFrame({\n",
    "    \"Feature\": features,\n",
    "    \"Importance\": importances\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Plot top 20 features\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x=\"Importance\", y=\"Feature\", data=feat_imp.head(20), palette=\"viridis\")\n",
    "plt.title(\"Top 20 Feature Importances - XGBoost\", fontsize=14)\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
